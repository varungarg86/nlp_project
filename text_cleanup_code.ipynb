{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzFGZFtLgOrT"
   },
   "source": [
    "#Demonstration: Building an NLP Pipeline for Multilingual Tweet Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jsiBJwwfcrh"
   },
   "source": [
    "##Scenario:\n",
    "\n",
    "- Alex, an NLP engineer at a social media company, receives a messy dataset of multilingual tweets filled with emojis, emoticons, hashtags, mentions, and noise. Since the goal is to analyze only English tweets, manually cleaning and filtering them would be too time-consuming. To streamline this, she builds an automated NLP pipeline that detects language, filters for English, removes noise, lowercases text, eliminates stopwords, tokenizes content, and applies techniques like BPE, one-hot encoding, and TF-IDFâ€”turning raw tweets into clean, structured data ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xroh_v5TbLfP"
   },
   "source": [
    "##Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1f2BAOXc2kh",
    "outputId": "a07bf0a5-30de-42fe-f026-2cad951fe603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.0.9)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.8.11)\n",
      "Requirement already satisfied: ftfy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (6.3.1)\n",
      "Requirement already satisfied: contractions in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: emoji in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.15.0)\n",
      "Requirement already satisfied: tokenizers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.22.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.11.10)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tokenizers) (0.36.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: anyascii in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk langdetect spacy ftfy contractions emoji tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvqg4kO_bK5d",
    "outputId": "7f5a77a9-669f-4f94-f2ee-eae43d80648b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported and models downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varuniexpress/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varuniexpress/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/varuniexpress/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import ftfy\n",
    "import contractions\n",
    "\n",
    "# Language & Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Emojis & Emoticons\n",
    "import emoji\n",
    "\n",
    "# Byte Pair Encoding (BPE)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"âœ… Libraries imported and models downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXP5x3xrbQhZ"
   },
   "source": [
    "##Load and Show Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OY9_H2tkbRpf",
    "outputId": "caa4ad03-c132-4f21-bc08-c9852f66e93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Sample data:\n",
      "   username location  gender  age  \\\n",
      "0   user_1    India  Female   43   \n",
      "1   user_2   France    Male   26   \n",
      "2   user_3  Germany   Other   30   \n",
      "3   user_4  Germany  Female   38   \n",
      "4   user_5    Japan  Female   39   \n",
      "\n",
      "                                           tweet  \n",
      "0  Can't believe this happened... lol :D #fail ğŸ˜…  \n",
      "1                                     ä¿¡ã˜ã‚‰ã‚Œãªã„â€¦ ğŸ˜­ğŸ’”  \n",
      "2                 Das ist fantastisch ğŸ˜ğŸ’¯! #Liebe  \n",
      "3  Can't believe this happened... lol :D #fail ğŸ˜…  \n",
      "4                  Â¡Esto es perfecto! ğŸ˜ŠğŸ’ƒ #fiesta  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_dataset.csv\")\n",
    "print(\"ğŸ“„ Sample data:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESZKSFX3bTY3"
   },
   "source": [
    "##Detect Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zPV0EzKbU1c",
    "outputId": "5cf1c7af-8975-4bb2-de69-70d2175fb858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Count:\n",
      " language\n",
      "de    14\n",
      "id    14\n",
      "fr    13\n",
      "pt    13\n",
      "ja    12\n",
      "en     8\n",
      "ru     8\n",
      "hi     7\n",
      "es     6\n",
      "ur     4\n",
      "ar     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['language'] = df['tweet'].apply(lambda x: detect(x))\n",
    "print(\"Language Count:\\n\", df['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djf-AxBKbWrG"
   },
   "source": [
    "##Filter English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYKC__XIbXwN",
    "outputId": "c4771565-6ad2-46b3-d651-32642e40cb74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered English Tweets:\n",
      "                                            tweet\n",
      "0  Can't believe this happened... lol :D #fail ğŸ˜…\n",
      "1  Can't believe this happened... lol :D #fail ğŸ˜…\n",
      "2  Can't believe this happened... lol :D #fail ğŸ˜…\n",
      "3        I love this! ğŸ˜ Soooo good!!! #awesome ğŸ˜Š\n",
      "4  Can't believe this happened... lol :D #fail ğŸ˜…\n"
     ]
    }
   ],
   "source": [
    "df_en = df[df['language'] == 'en'].reset_index(drop=True)\n",
    "print(\"Filtered English Tweets:\\n\", df_en[['tweet']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgbLOkUobZpn"
   },
   "source": [
    "##Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJsTzE9jbavu",
    "outputId": "639093c0-801b-4572-dd46-caa9d94395a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Output :\n",
      " 0    can't believe this happened... lol :d #fail ğŸ˜…\n",
      "1    can't believe this happened... lol :d #fail ğŸ˜…\n",
      "2    can't believe this happened... lol :d #fail ğŸ˜…\n",
      "3          i love this! ğŸ˜ soooo good!!! #awesome ğŸ˜Š\n",
      "4    can't believe this happened... lol :d #fail ğŸ˜…\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_en['cleaned'] = df_en['tweet'].apply(lambda x: x.lower())\n",
    "print(\"Lowercased Output :\\n\", df_en['cleaned'].head())\n",
    "\n",
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyaBIfbTbcYA"
   },
   "source": [
    "##Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocHwci3XbdiC",
    "outputId": "f73734a4-a7f4-47dd-9775-9a8aee388d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post stopword removal:\n",
      " 0    ca n't believe happened ... lol : # fail ğŸ˜…\n",
      "1    ca n't believe happened ... lol : # fail ğŸ˜…\n",
      "2    ca n't believe happened ... lol : # fail ğŸ˜…\n",
      "3         love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š\n",
      "4    ca n't believe happened ... lol : # fail ğŸ˜…\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(\n",
    "    lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words])\n",
    ")\n",
    "print(\"Post stopword removal:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwcBeVJxbfsC"
   },
   "source": [
    "##Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7grunZzbguP",
    "outputId": "64c0112d-e2ec-401b-a46b-b21c8959f7f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " 0    [ca, n't, believe, happened, ..., lol, :, #, f...\n",
      "1    [ca, n't, believe, happened, ..., lol, :, #, f...\n",
      "2    [ca, n't, believe, happened, ..., lol, :, #, f...\n",
      "3    [love, !, ğŸ˜, soooo, good, !, !, !, #, awesome, ğŸ˜Š]\n",
      "4    [ca, n't, believe, happened, ..., lol, :, #, f...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_en['tokens'] = df_en['cleaned'].apply(word_tokenize)\n",
    "print(\"Tokens:\\n\", df_en['tokens'].head())\n",
    "\n",
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>Porter_Stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca n't believe happened ... lol : # fail ğŸ˜…</td>\n",
       "      <td>ca n't believ happen ... lol : # fail ğŸ˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca n't believe happened ... lol : # fail ğŸ˜…</td>\n",
       "      <td>ca n't believ happen ... lol : # fail ğŸ˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca n't believe happened ... lol : # fail ğŸ˜…</td>\n",
       "      <td>ca n't believ happen ... lol : # fail ğŸ˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š</td>\n",
       "      <td>love ! ğŸ˜ soooo good ! ! ! # awesom ğŸ˜Š</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca n't believe happened ... lol : # fail ğŸ˜…</td>\n",
       "      <td>ca n't believ happen ... lol : # fail ğŸ˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š</td>\n",
       "      <td>love ! ğŸ˜ soooo good ! ! ! # awesom ğŸ˜Š</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned  \\\n",
       "0  ca n't believe happened ... lol : # fail ğŸ˜…   \n",
       "1  ca n't believe happened ... lol : # fail ğŸ˜…   \n",
       "2  ca n't believe happened ... lol : # fail ğŸ˜…   \n",
       "3       love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š   \n",
       "4  ca n't believe happened ... lol : # fail ğŸ˜…   \n",
       "5       love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š   \n",
       "\n",
       "                               Porter_Stem  \n",
       "0  ca n't believ happen ... lol : # fail ğŸ˜…  \n",
       "1  ca n't believ happen ... lol : # fail ğŸ˜…  \n",
       "2  ca n't believ happen ... lol : # fail ğŸ˜…  \n",
       "3     love ! ğŸ˜ soooo good ! ! ! # awesom ğŸ˜Š  \n",
       "4  ca n't believ happen ... lol : # fail ğŸ˜…  \n",
       "5     love ! ğŸ˜ soooo good ! ! ! # awesom ğŸ˜Š  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "df_en['Porter_Stem'] = df_en['cleaned'].apply(\n",
    "    lambda text: \" \".join([porter.stem(word) for word in word_tokenize(text.lower())])\n",
    ")\n",
    "df_en[['cleaned', 'Porter_Stem']].head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    can not believe happen ... lol : # fail ğŸ˜…\n",
       "1    can not believe happen ... lol : # fail ğŸ˜…\n",
       "2    can not believe happen ... lol : # fail ğŸ˜…\n",
       "3        love ! ğŸ˜ soooo good ! ! ! # awesome ğŸ˜Š\n",
       "4    can not believe happen ... lol : # fail ğŸ˜…\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import spacy\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(\n",
    "    lambda text: \" \".join([token.lemma_ for token in nlp_spacy(text)])\n",
    ")\n",
    "df_en['cleaned'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb08LUz4bie5"
   },
   "source": [
    "##Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQFrzba8bj7K",
    "outputId": "8da6cf94-6c57-454d-c3f2-3822d16613fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ğŸ§  BPE token sample:\n",
      " 0    [can, not, believe, happen, ..., lol, :, #, fa...\n",
      "1    [can, not, believe, happen, ..., lol, :, #, fa...\n",
      "2    [can, not, believe, happen, ..., lol, :, #, fa...\n",
      "3    [love, !, ğŸ˜, soooo, good, !, !, !, #, awesome, ğŸ˜Š]\n",
      "4    [can, not, believe, happen, ..., lol, :, #, fa...\n",
      "Name: bpe_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Prepare file for BPE training\n",
    "with open(\"bpe_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in df_en['cleaned']:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Train and tokenize using BPE\n",
    "tokenizer = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(special_tokens=[\"<unk>\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train(files=[\"bpe_corpus.txt\"], trainer=trainer)\n",
    "df_en['bpe_tokens'] = df_en['cleaned'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "\n",
    "print(\"ğŸ§  BPE token sample:\\n\", df_en['bpe_tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6hxffSMbmOf"
   },
   "source": [
    "##One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yo_wMzatbn4k",
    "outputId": "e6421f8f-5ade-4cd2-f286-4a5dc9603bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ One-Hot for 1st Tweet (shape): (10, 17)\n"
     ]
    }
   ],
   "source": [
    "# Flatten all words into one list\n",
    "all_words = list(set([word for tokens in df_en['tokens'] for word in tokens]))\n",
    "\n",
    "# Create encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit([[word] for word in all_words])\n",
    "\n",
    "# Encode first tweet for demo\n",
    "encoded_sample = encoder.transform([[w] for w in df_en['tokens'][0]])\n",
    "print(\"ğŸ¯ One-Hot for 1st Tweet (shape):\", encoded_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw_1a2sAbpu5"
   },
   "source": [
    "##TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZygVJ2Nbq8q",
    "outputId": "b220897b-f264-4d5c-89ee-c91999c54e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ TF-IDF Shape: (8, 10)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df_en['cleaned'])\n",
    "print(\"ğŸ“ˆ TF-IDF Shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnc_M4uHbszW"
   },
   "source": [
    "##Remove Noise (URLs, Mentions, Hashtags, Punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AslUG7-bt1o",
    "outputId": "238e39fd-d2e6-4252-beef-9b5d5194bfae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post noise removal:\n",
      " 0    can not believe happen  lol   fail ğŸ˜…\n",
      "1    can not believe happen  lol   fail ğŸ˜…\n",
      "2    can not believe happen  lol   fail ğŸ˜…\n",
      "3        love  ğŸ˜ soooo good     awesome ğŸ˜Š\n",
      "4    can not believe happen  lol   fail ğŸ˜…\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                # Mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                # Hashtags\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)  # Punctuation\n",
    "    return text\n",
    "\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(remove_noise)\n",
    "print(\"Post noise removal:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3srYqzhIbvvY"
   },
   "source": [
    "##Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1F2Bf10bw5h",
    "outputId": "a1f85acd-2b0a-4359-e9a0-adf404598f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis removed:\n",
      " 0    can not believe happen  lol   fail \n",
      "1    can not believe happen  lol   fail \n",
      "2    can not believe happen  lol   fail \n",
      "3         love   soooo good     awesome \n",
      "4    can not believe happen  lol   fail \n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(remove_emojis)\n",
    "print(\"Emojis removed:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QQ6xGYFbyuF"
   },
   "source": [
    "##Remove Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPcd7OXib47J",
    "outputId": "4e6833e7-a0a8-459d-d2d4-4fc38518b2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons removed:\n",
      " 0    can not believe happen lol fail\n",
      "1    can not believe happen lol fail\n",
      "2    can not believe happen lol fail\n",
      "3            love soooo good awesome\n",
      "4    can not believe happen lol fail\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons = {\":)\", \":(\", \":D\", \":-)\", \":-(\", \":'(\", \":-D\", \";)\", \":P\", \":-P\"}\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    return ' '.join([word for word in text.split() if word not in emoticons])\n",
    "\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(remove_emoticons)\n",
    "print(\"Emoticons removed:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ6mg1Gmb0yw"
   },
   "source": [
    "##Handle Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJkfvG5Pb2E8",
    "outputId": "15863b28-73f4-4b06-8b6f-9328f01e97d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions handled:\n",
      " 0    can not believe happen lol fail\n",
      "1    can not believe happen lol fail\n",
      "2    can not believe happen lol fail\n",
      "3            love soooo good awesome\n",
      "4    can not believe happen lol fail\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_en['cleaned'] = df_en['cleaned'].apply(lambda x: contractions.fix(x))\n",
    "print(\"Contractions handled:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AALB_Oo6b7_A"
   },
   "source": [
    "##Normalization (Accents, Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW0Q9oIYb9P3",
    "outputId": "871a02be-20ec-46dc-f247-302520e0f116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§½ Normalized text:\n",
      " 0    can not believe happen lol fail\n",
      "1    can not believe happen lol fail\n",
      "2    can not believe happen lol fail\n",
      "3            love soooo good awesome\n",
      "4    can not believe happen lol fail\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = ftfy.fix_text(text)  # Fix messy text\n",
    "    return text\n",
    "\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(normalize_text)\n",
    "print(\"ğŸ§½ Normalized text:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXuS-VW4b_ET"
   },
   "source": [
    "##Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N60tAGMacASs",
    "outputId": "30112bff-e997-4793-9d0d-66235a75dbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode normalized:\n",
      " 0    can not believe happen lol fail\n",
      "1    can not believe happen lol fail\n",
      "2    can not believe happen lol fail\n",
      "3            love soooo good awesome\n",
      "4    can not believe happen lol fail\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def unicode_normalize(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "df_en['cleaned'] = df_en['cleaned'].apply(unicode_normalize)\n",
    "print(\"Unicode normalized:\\n\", df_en['cleaned'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By0c0e0OcCiz"
   },
   "source": [
    "##Display Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGIDaSljcDy6",
    "outputId": "e9c5fb91-3e0f-427b-a86d-b7955b552494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Processed Output Sample:\n",
      "                                            tweet  \\\n",
      "0  Can't believe this happened... lol :D #fail ğŸ˜…   \n",
      "1  Can't believe this happened... lol :D #fail ğŸ˜…   \n",
      "2  Can't believe this happened... lol :D #fail ğŸ˜…   \n",
      "3        I love this! ğŸ˜ Soooo good!!! #awesome ğŸ˜Š   \n",
      "4  Can't believe this happened... lol :D #fail ğŸ˜…   \n",
      "\n",
      "                           cleaned  \\\n",
      "0  can not believe happen lol fail   \n",
      "1  can not believe happen lol fail   \n",
      "2  can not believe happen lol fail   \n",
      "3          love soooo good awesome   \n",
      "4  can not believe happen lol fail   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ca, n't, believe, happened, ..., lol, :, #, f...   \n",
      "1  [ca, n't, believe, happened, ..., lol, :, #, f...   \n",
      "2  [ca, n't, believe, happened, ..., lol, :, #, f...   \n",
      "3  [love, !, ğŸ˜, soooo, good, !, !, !, #, awesome, ğŸ˜Š]   \n",
      "4  [ca, n't, believe, happened, ..., lol, :, #, f...   \n",
      "\n",
      "                                          bpe_tokens  \n",
      "0  [can, not, believe, happen, ..., lol, :, #, fa...  \n",
      "1  [can, not, believe, happen, ..., lol, :, #, fa...  \n",
      "2  [can, not, believe, happen, ..., lol, :, #, fa...  \n",
      "3  [love, !, ğŸ˜, soooo, good, !, !, !, #, awesome, ğŸ˜Š]  \n",
      "4  [can, not, believe, happen, ..., lol, :, #, fa...  \n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… Final Processed Output Sample:\\n\", df_en[['tweet', 'cleaned', 'tokens', 'bpe_tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CshMQgRcFUU"
   },
   "source": [
    "##Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaQmaw4TcGfv",
    "outputId": "e0ab3db6-7962-4bbc-c4d3-e9854c95367b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Cleaned dataset saved as 'cleaned_multilingual_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "df_en.to_csv(\"cleaned_multilingual_tweets.csv\", index=False)\n",
    "print(\"ğŸ“ Cleaned dataset saved as 'cleaned_multilingual_tweets.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
